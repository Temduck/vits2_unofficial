{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for ISSAI TTS speakers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "def write_records_to_json(records, filename):\n",
    "  \"\"\"\n",
    "  Writes a dictionary of records to a JSON file.\n",
    "\n",
    "  Args:\n",
    "    records: A dictionary where keys are speaker IDs and values are file paths.\n",
    "    filename: The name of the JSON file to write to.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    json.dump(records, f)\n",
    "\n",
    "def read_records_from_json(filename):\n",
    "  \"\"\"\n",
    "  Reads a dictionary of records from a JSON file.\n",
    "\n",
    "  Args:\n",
    "    filename: The name of the JSON file to read from.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of records.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(filename, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "speaker_metadata = read_records_from_json(\"../../../metadata/speaker_metadata.json\")\n",
    "speaker_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"/home/temduck/vits2_unofficial/\"\n",
    "config = \"../config.yaml\"\n",
    "symlink = \"F1\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here ISSAI dataset speakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_datasets = {}\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id] = pd.read_csv(dir_data+speaker_metadata[speaker_id],\n",
    "                                                names=[\"file\", \"text\"], header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA speakers text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['cleaned_text']\n",
    "        for character in text:\n",
    "            charset[character.lower()] += 1\n",
    "    return charset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "from phonemizer.backend import EspeakBackend\n",
    "import text_normalizer as nums_normalizer\n",
    "from symbols import cyrillic_mapping\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "speaker_metadata = read_records_from_json(\"../../../metadata/speaker_metadata.json\")\n",
    "speakers_datasets = {}\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id] = pd.read_csv(dir_data+speaker_metadata[speaker_id],\n",
    "                                                names=[\"file\", \"text\"], header=0)\n",
    "\n",
    "\n",
    "_numerals = [\n",
    "    (re.compile(x[0]), x[1])\n",
    "    for x in [\n",
    "        (r'\\b(3[01]|[12][0-9]|[1-9])\\s(қаңтар|ақпан|наурыз|сәуір|мамыр|маусым|шілде|тамыз|қыркүйек|қазан|қараша|желтоқсан)', '_replace_nums_pair_word'), # nums pair kazakh month\n",
    "        (r'\\b\\d{4}\\s(жыл)', '_replace_nums_pair_word'), # nums pair kazakh year        \n",
    "        (r'\\b\\d{1,3}\\b', '_replace_nums'), # hundreds \n",
    "        (r'[а-яА-ЯӘәҒғҚқҢңӨөҰұҮүҺһІі]+\\d+', '_remove_nums'), # kazakh word with digit\n",
    "        (r'\\d+[а-яА-ЯӘәҒғҚқҢңӨөҰұҮүҺһІі]+', '_remove_nums'), # digit with kazakh word \n",
    "        (r'\\d+-[інші|ыншы|сыншы|ші|шы]', '_replace_ordinal_nums'),  # ordianal numerals with suffix \n",
    "        (r'\\d+-(ден|тан|тен)', '_replace_group_nums') # group numerals with suffix \n",
    "    ]\n",
    "]\n",
    "\n",
    "_issaitts_trash = [\n",
    "    ((re.compile(\"%s\" % x[0], re.IGNORECASE), x[1]))\n",
    "    for x in [\n",
    "        ('–|—|−|－', '-'),\n",
    "        (\"\\n|noise|ʨ|ɕ|»|–|«|—|̆|“|”|…|−|－|●\", '')\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    for regex, replacement_func_name in _numerals:\n",
    "        replacement_func = getattr(nums_normalizer, replacement_func_name)\n",
    "        text = regex.sub(replacement_func, text)\n",
    "    return text\n",
    "\n",
    "def remove_trash(text):\n",
    "    for regex, replacement in _issaitts_trash:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def kazakh_cleaners_issaitts(text):\n",
    "    \"\"\"Pipeline for Kazakh tts speakers datasets text, including num2words, + punctuation + g2p\"\"\"\n",
    "    table = str.maketrans(dict.fromkeys('#$%&\\'()*+/:;<=>@[\\\\]^_`{|}~—…\"«»“”'))\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = text.translate(table)\n",
    "    text = remove_trash(text)\n",
    "    text = ''.join(cyrillic_mapping.get(char, char) for char in text)\n",
    "    return text \n",
    "\n",
    "\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"normalized_text\"] = speakers_datasets[speaker_id][\"text\"].progress_apply(kazakh_cleaners_issaitts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torchtext\n",
    "\n",
    "speaker_id = \"M2\"\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "data = speakers_datasets[speaker_id]\n",
    "text_norm = data[\"text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab_{speaker_id}.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['normalized_text']\n",
    "        for character in text:\n",
    "            charset[character.lower()] += 1\n",
    "    return charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer import phonemize\n",
    "from typing import List\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer.backend import EspeakBackend\n",
    "separator = Separator(word=\"<space>\", phone=\" \")\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“”'\n",
    "_preserved_symbols_re = re.compile(rf\"[{_punctuation}]|<.*?>\")\n",
    "backend = EspeakBackend(language=\"kk\", preserve_punctuation=True, with_stress=True, punctuation_marks=_preserved_symbols_re)\n",
    "def phonemize_text(text: List[str] | str, *args, language=\"kk\", **kwargs):\n",
    "    return phonemize(text, language=language, backend=\"espeak\", separator=separator, strip=True, preserve_punctuation=True, punctuation_marks=_preserved_symbols_re, with_stress=True, njobs=8)\n",
    "\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"cleaned_text\"] = speakers_datasets[speaker_id][\"normalized_text\"].progress_apply(lambda text: backend.phonemize([text], strip=True, separator=separator)[0])\n",
    "# clean_text  = phonemize_text(speakers_datasets[speaker_id][\"normalized_text\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torchtext\n",
    "tokenizer('e n d e ʃ ˈe<space>q ɑ ɫ ˈɑ j<space>m e n ˈɪ<space>ʒ ˈʊ m ə s q ɑ<space>ˈɑ ɫ d ə?', Vocab, [\"add_spaces\"], language=\"kk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"cleaned_text\"] = speakers_datasets[speaker_id][\"cleaned_text\"].progress_apply(lambda text: tokenizer(text, Vocab, [\"add_spaces\"], language=\"kk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(speakers_datasets.values()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    return \"\\t\".join(map(str, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"tokens\"] = speakers_datasets[speaker_id][\"cleaned_text\"].progress_apply(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = \"M2\"\n",
    "data = speakers_datasets[speaker_id]\n",
    "data = data[[\"file\", \"tokens\"]]\n",
    "# data[\"text\"] =  data[\"text\"].str.strip()\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_train_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_val_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_test_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_s = [\"file\", \"normalized_text\", \"cleaned_text\", \"tokens\"] \n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id].to_csv(f\"/home/temduck/vits2_unofficial/metadata/{speaker_id}_file_nomalized_cleaned_tokens.csv\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multispeaker dataset prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../metadata/F2_file_nomalized_cleaned_tokens.csv\n",
      "../../../metadata/M1_file_nomalized_cleaned_tokens.csv\n",
      "../../../metadata/M2_file_nomalized_cleaned_tokens.csv\n",
      "../../../metadata/F3_file_nomalized_cleaned_tokens.csv\n",
      "../../../metadata/F1_file_nomalized_cleaned_tokens.csv\n"
     ]
    }
   ],
   "source": [
    "! find ../../../metadata/ -type f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = ! find ../../../metadata/ -type f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../metadata/F1_file_nomalized_cleaned_tokens.csv',\n",
       " '../../../metadata/F2_file_nomalized_cleaned_tokens.csv',\n",
       " '../../../metadata/F3_file_nomalized_cleaned_tokens.csv',\n",
       " '../../../metadata/M1_file_nomalized_cleaned_tokens.csv',\n",
       " '../../../metadata/M2_file_nomalized_cleaned_tokens.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "speaker_id = 1\n",
    "df = pd.read_csv(sorted(metadata)[speaker_id], sep=\"|\", names=[\"file\", \"text\",\"normalized_text\", \"cleaned_text\", \"tokens\"])\n",
    "df[\"sid\"] = speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F2/Audio/inform_1266_00.wav</td>\n",
       "      <td>ақтөбе облысының әкімі оңдасын оразалин өнеркә...</td>\n",
       "      <td>ақтөбе облысының әкімі оңдасын оразалин өнеркә...</td>\n",
       "      <td>ɑ q t ɵ b ˈe &lt;space&gt; ˈo b ɫ ə s ə n ə ŋ &lt;space...</td>\n",
       "      <td>2\\t7\\t17\\t9\\t37\\t23\\t22\\t4\\t31\\t23\\t15\\t6\\t13\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F2/Audio/inform_725_08.wav</td>\n",
       "      <td>Түнде бұрқасын көтеріледі.\\n</td>\n",
       "      <td>түнде бұрқасын көтеріледі.</td>\n",
       "      <td>t u n d ˈe &lt;space&gt; b ʊ ɾ q ˈɑ s ə n &lt;space&gt; k ...</td>\n",
       "      <td>2\\t9\\t39\\t8\\t14\\t22\\t4\\t23\\t41\\t11\\t17\\t10\\t13...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F2/Audio/inform_2460_21.wav</td>\n",
       "      <td>елбасы нұр-сұлтан назарбаев жолаушылар ұшағыны...</td>\n",
       "      <td>елбасы нұр-сұлтан назарбаев жолаушылар ұшағыны...</td>\n",
       "      <td>e ɫ b ˈɑ s ə &lt;space&gt; n ˈʊ ɾ s ʊ ɫ t ˈɑ n &lt;spac...</td>\n",
       "      <td>2\\t12\\t15\\t23\\t10\\t13\\t6\\t4\\t8\\t40\\t11\\t13\\t41...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F2/Audio/inform_4428_02.wav</td>\n",
       "      <td>олардың плей-офф кезеңіне шығуға мүмкіндігі зо...</td>\n",
       "      <td>олардың плей-офф кезеңіне шығуға мүмкіндігі зор.</td>\n",
       "      <td>o ɫ ˈɑ ɾ d ə ŋ &lt;space&gt; p l ˈe j ˈo f f &lt;space&gt;...</td>\n",
       "      <td>2\\t32\\t15\\t10\\t11\\t14\\t6\\t29\\t4\\t28\\t35\\t22\\t1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F2/Audio/inform_968_09.wav</td>\n",
       "      <td>бұл оқу орындарында жыл сайын түлектерді даярл...</td>\n",
       "      <td>бұл оқу орындарында жыл сайын түлектерді даярл...</td>\n",
       "      <td>b ˈʊ ɫ &lt;space&gt; ˈo q w &lt;space&gt; ˈo ɾ ə n d ɑ ɾ ə...</td>\n",
       "      <td>2\\t23\\t40\\t15\\t4\\t31\\t17\\t26\\t4\\t31\\t11\\t6\\t8\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file  \\\n",
       "0  F2/Audio/inform_1266_00.wav   \n",
       "1   F2/Audio/inform_725_08.wav   \n",
       "2  F2/Audio/inform_2460_21.wav   \n",
       "3  F2/Audio/inform_4428_02.wav   \n",
       "4   F2/Audio/inform_968_09.wav   \n",
       "\n",
       "                                                text  \\\n",
       "0  ақтөбе облысының әкімі оңдасын оразалин өнеркә...   \n",
       "1                       Түнде бұрқасын көтеріледі.\\n   \n",
       "2  елбасы нұр-сұлтан назарбаев жолаушылар ұшағыны...   \n",
       "3  олардың плей-офф кезеңіне шығуға мүмкіндігі зо...   \n",
       "4  бұл оқу орындарында жыл сайын түлектерді даярл...   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  ақтөбе облысының әкімі оңдасын оразалин өнеркә...   \n",
       "1                         түнде бұрқасын көтеріледі.   \n",
       "2  елбасы нұр-сұлтан назарбаев жолаушылар ұшағыны...   \n",
       "3   олардың плей-офф кезеңіне шығуға мүмкіндігі зор.   \n",
       "4  бұл оқу орындарында жыл сайын түлектерді даярл...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  ɑ q t ɵ b ˈe <space> ˈo b ɫ ə s ə n ə ŋ <space...   \n",
       "1  t u n d ˈe <space> b ʊ ɾ q ˈɑ s ə n <space> k ...   \n",
       "2  e ɫ b ˈɑ s ə <space> n ˈʊ ɾ s ʊ ɫ t ˈɑ n <spac...   \n",
       "3  o ɫ ˈɑ ɾ d ə ŋ <space> p l ˈe j ˈo f f <space>...   \n",
       "4  b ˈʊ ɫ <space> ˈo q w <space> ˈo ɾ ə n d ɑ ɾ ə...   \n",
       "\n",
       "                                              tokens  sid  \n",
       "0  2\\t7\\t17\\t9\\t37\\t23\\t22\\t4\\t31\\t23\\t15\\t6\\t13\\...    1  \n",
       "1  2\\t9\\t39\\t8\\t14\\t22\\t4\\t23\\t41\\t11\\t17\\t10\\t13...    1  \n",
       "2  2\\t12\\t15\\t23\\t10\\t13\\t6\\t4\\t8\\t40\\t11\\t13\\t41...    1  \n",
       "3  2\\t32\\t15\\t10\\t11\\t14\\t6\\t29\\t4\\t28\\t35\\t22\\t1...    1  \n",
       "4  2\\t23\\t40\\t15\\t4\\t31\\t17\\t26\\t4\\t31\\t11\\t6\\t8\\...    1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sid_df(speaker_id):\n",
    "    df = pd.read_csv(sorted(metadata)[speaker_id], sep=\"|\", names=[\"file\", \"text\",\"normalized_text\", \"cleaned_text\", \"tokens\"])\n",
    "    df[\"sid\"] = speaker_id\n",
    "    return df[[\"file\", \"sid\", \"tokens\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([get_sid_df(i) for i in range(5)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 100\n",
    "n_test = 500\n",
    "speaker_id = \"issai_base_file_sid_tokens\"\n",
    "data = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"../filelists/{}_train_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"../filelists/{}_val_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"../filelists/{}_test_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
