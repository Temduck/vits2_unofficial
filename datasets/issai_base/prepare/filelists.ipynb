{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare filelists for ISSAI TTS speakers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "def write_records_to_json(records, filename):\n",
    "  \"\"\"\n",
    "  Writes a dictionary of records to a JSON file.\n",
    "\n",
    "  Args:\n",
    "    records: A dictionary where keys are speaker IDs and values are file paths.\n",
    "    filename: The name of the JSON file to write to.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(filename, 'w') as f:\n",
    "    json.dump(records, f)\n",
    "\n",
    "def read_records_from_json(filename):\n",
    "  \"\"\"\n",
    "  Reads a dictionary of records from a JSON file.\n",
    "\n",
    "  Args:\n",
    "    filename: The name of the JSON file to read from.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of records.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(filename, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "speaker_metadata = read_records_from_json(\"../../../metadata/speaker_metadata.json\")\n",
    "speaker_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md\n",
    "dir_data = \"/home/temduck/vits2_unofficial/\"\n",
    "config = \"../config.yaml\"\n",
    "symlink = \"F1\"\n",
    "n_val = 100\n",
    "n_test = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hyperparameters from config file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.hparams import get_hparams_from_file\n",
    "\n",
    "hps = get_hparams_from_file(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset\n",
    "\n",
    "Here ISSAI dataset speakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_datasets = {}\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id] = pd.read_csv(dir_data+speaker_metadata[speaker_id],\n",
    "                                                names=[\"file\", \"text\"], header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA speakers text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['cleaned_text']\n",
    "        for character in text:\n",
    "            charset[character.lower()] += 1\n",
    "    return charset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "from phonemizer.backend import EspeakBackend\n",
    "import text_normalizer as nums_normalizer\n",
    "from symbols import cyrillic_mapping\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "speaker_metadata = read_records_from_json(\"../../../metadata/speaker_metadata.json\")\n",
    "speakers_datasets = {}\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id] = pd.read_csv(dir_data+speaker_metadata[speaker_id],\n",
    "                                                names=[\"file\", \"text\"], header=0)\n",
    "\n",
    "\n",
    "_numerals = [\n",
    "    (re.compile(x[0]), x[1])\n",
    "    for x in [\n",
    "        (r'\\b(3[01]|[12][0-9]|[1-9])\\s(қаңтар|ақпан|наурыз|сәуір|мамыр|маусым|шілде|тамыз|қыркүйек|қазан|қараша|желтоқсан)', '_replace_nums_pair_word'), # nums pair kazakh month\n",
    "        (r'\\b\\d{4}\\s(жыл)', '_replace_nums_pair_word'), # nums pair kazakh year        \n",
    "        (r'\\b\\d{1,3}\\b', '_replace_nums'), # hundreds \n",
    "        (r'[а-яА-ЯӘәҒғҚқҢңӨөҰұҮүҺһІі]+\\d+', '_remove_nums'), # kazakh word with digit\n",
    "        (r'\\d+[а-яА-ЯӘәҒғҚқҢңӨөҰұҮүҺһІі]+', '_remove_nums'), # digit with kazakh word \n",
    "        (r'\\d+-[інші|ыншы|сыншы|ші|шы]', '_replace_ordinal_nums'),  # ordianal numerals with suffix \n",
    "        (r'\\d+-(ден|тан|тен)', '_replace_group_nums') # group numerals with suffix \n",
    "    ]\n",
    "]\n",
    "\n",
    "_issaitts_trash = [\n",
    "    ((re.compile(\"%s\" % x[0], re.IGNORECASE), x[1]))\n",
    "    for x in [\n",
    "        ('–|—|−|－', '-'),\n",
    "        (\"\\n|noise|ʨ|ɕ|»|–|«|—|̆|“|”|…|−|－|●\", '')\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    for regex, replacement_func_name in _numerals:\n",
    "        replacement_func = getattr(nums_normalizer, replacement_func_name)\n",
    "        text = regex.sub(replacement_func, text)\n",
    "    return text\n",
    "\n",
    "def remove_trash(text):\n",
    "    for regex, replacement in _issaitts_trash:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def kazakh_cleaners_issaitts(text):\n",
    "    \"\"\"Pipeline for Kazakh tts speakers datasets text, including num2words, + punctuation + g2p\"\"\"\n",
    "    table = str.maketrans(dict.fromkeys('#$%&\\'()*+/:;<=>@[\\\\]^_`{|}~—…\"«»“”'))\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = text.translate(table)\n",
    "    text = remove_trash(text)\n",
    "    text = ''.join(cyrillic_mapping.get(char, char) for char in text)\n",
    "    return text \n",
    "\n",
    "\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"normalized_text\"] = speakers_datasets[speaker_id][\"text\"].progress_apply(kazakh_cleaners_issaitts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaners\n",
    "\n",
    "It may take a while, so better to preprocess the text and save it to a file in advance.\n",
    "\n",
    "**Note** `phonemize_text` takes the longest time.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of tokenize_text\n",
    "text_cleaners = hps.data.text_cleaners\n",
    "\n",
    "token_idx = text_cleaners.index(\"tokenize_text\")\n",
    "token_cleaners = text_cleaners[token_idx:]\n",
    "print(token_cleaners)\n",
    "\n",
    "\n",
    "# Extract phonemize_text\n",
    "def separate_text_cleaners(text_cleaners):\n",
    "    final_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for cleaner in text_cleaners:\n",
    "        if cleaner == \"phonemize_text\":\n",
    "            if temp_list:\n",
    "                final_list.append(temp_list)\n",
    "            final_list.append([cleaner])\n",
    "            temp_list = []\n",
    "        else:\n",
    "            temp_list.append(cleaner)\n",
    "\n",
    "    if temp_list:\n",
    "        final_list.append(temp_list)\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "text_cleaners = text_cleaners[:token_idx]\n",
    "text_cleaners = separate_text_cleaners(text_cleaners)\n",
    "print(text_cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torchtext\n",
    "\n",
    "speaker_id = \"M2\"\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "data = speakers_datasets[speaker_id]\n",
    "text_norm = data[\"text\"].tolist()\n",
    "for cleaners in text_cleaners:\n",
    "    print(f\"Cleaning with {cleaners} ...\")\n",
    "    if cleaners[0] == \"phonemize_text\":\n",
    "        text_norm = tokenizer(text_norm, Vocab, cleaners, language=hps.data.language)\n",
    "    else:\n",
    "        for idx, text in enumerate(text_norm):\n",
    "            temp = tokenizer(text, Vocab, cleaners, language=hps.data.language)\n",
    "            text_norm[idx] = temp\n",
    "\n",
    "data = data.assign(cleaned_text=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab_{speaker_id}.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token cleaners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save train, val, test filelists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['normalized_text']\n",
    "        for character in text:\n",
    "            charset[character.lower()] += 1\n",
    "    return charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer import phonemize\n",
    "from typing import List\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer.backend import EspeakBackend\n",
    "separator = Separator(word=\"<space>\", phone=\" \")\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“”'\n",
    "_preserved_symbols_re = re.compile(rf\"[{_punctuation}]|<.*?>\")\n",
    "backend = EspeakBackend(language=\"kk\", preserve_punctuation=True, with_stress=True, punctuation_marks=_preserved_symbols_re)\n",
    "def phonemize_text(text: List[str] | str, *args, language=\"kk\", **kwargs):\n",
    "    return phonemize(text, language=language, backend=\"espeak\", separator=separator, strip=True, preserve_punctuation=True, punctuation_marks=_preserved_symbols_re, with_stress=True, njobs=8)\n",
    "\n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"cleaned_text\"] = speakers_datasets[speaker_id][\"normalized_text\"].progress_apply(lambda text: backend.phonemize([text], strip=True, separator=separator)[0])\n",
    "# clean_text  = phonemize_text(speakers_datasets[speaker_id][\"normalized_text\"].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "import torchtext\n",
    "tokenizer('e n d e ʃ ˈe<space>q ɑ ɫ ˈɑ j<space>m e n ˈɪ<space>ʒ ˈʊ m ə s q ɑ<space>ˈɑ ɫ d ə?', Vocab, [\"add_spaces\"], language=\"kk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"cleaned_text\"] = speakers_datasets[speaker_id][\"cleaned_text\"].progress_apply(lambda text: tokenizer(text, Vocab, [\"add_spaces\"], language=\"kk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(speakers_datasets.values()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from utils.task import load_vocab, save_vocab\n",
    "from text.symbols import special_symbols, UNK_ID\n",
    "from typing import List\n",
    "\n",
    "def yield_tokens(cleaned_text: List[str]):\n",
    "    for text in cleaned_text:\n",
    "        yield text.split()\n",
    "\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "vocab = build_vocab_from_iterator(yield_tokens(text_norm), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_ID)\n",
    "\n",
    "vocab_file = f\"../vocab.txt\"\n",
    "save_vocab(vocab, vocab_file)\n",
    "\n",
    "vocab = load_vocab(vocab_file)\n",
    "print(f\"Size of vocabulary: {len(vocab)}\")\n",
    "print(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import detokenizer\n",
    "\n",
    "text_norm = data[\"cleaned_text\"].tolist()\n",
    "for idx, text in enumerate(text_norm):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    text_norm[idx] = temp\n",
    "\n",
    "text_norm = [\"\\t\".join(map(str, text)) for text in text_norm]\n",
    "data = data.assign(tokens=text_norm)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    temp = tokenizer(text, vocab, token_cleaners, language=hps.data.language)\n",
    "    assert UNK_ID not in temp, f\"Found unknown symbol:\\n{text}\\n{detokenizer(temp)}\"\n",
    "    return \"\\t\".join(map(str, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id][\"tokens\"] = speakers_datasets[speaker_id][\"cleaned_text\"].progress_apply(get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = \"M2\"\n",
    "data = speakers_datasets[speaker_id]\n",
    "data = data[[\"file\", \"tokens\"]]\n",
    "# data[\"text\"] =  data[\"text\"].str.strip()\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_train = data.iloc[n_val + n_test:]\n",
    "data_val = data.iloc[:n_val]\n",
    "data_test = data.iloc[n_val: n_val + n_test]\n",
    "\n",
    "data_train.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_train_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_val.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_val_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)\n",
    "data_test.to_csv(\"/home/temduck/vits2_unofficial/datasets/issai_base/filelists/issai_speakers/{}_test_filelist.txt\".format(speaker_id), sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_s = [\"file\", \"normalized_text\", \"cleaned_text\", \"tokens\"] \n",
    "for speaker_id in speaker_metadata.keys():\n",
    "    speakers_datasets[speaker_id].to_csv(f\"/home/temduck/vits2_unofficial/metadata/{speaker_id}_file_nomalized_cleaned_tokens.csv\", sep=\"|\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naturalspeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
